### Llama Factory 训练数据整理指南

------

#### **1. 训练数据格式要求**

参考链接：[LLaMA-Factory 数据格式说明](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md)

- **数据格式**

  - 微调数据需整理成以下两种格式之一：
    - **Alpaca格式**
    - **ShareGPT格式**（训练`function calling`功能时必须使用此格式）

- **模型类型区分**

  - Instruct 模型（单轮对话）
    - 适用于单轮指令跟随场景，字段定义简单。

  - Chat 模型（多轮对话）
    - 适用于多轮对话场景，需注意数据字段区别。

- **注意事项**

  - 使用 Llama Factory 训练时，务必选择与数据格式对应的正确模型模板。

------

#### **2. 具体数据格式**

##### **2.1 Alpaca 格式**

指令监督微调数据集的标准格式之一。

**样例数据集：**

在指令监督微调时，`instruction` 列对应的内容会与 `input` 列对应的内容拼接后作为人类指令，即人类指令为 `instruction\ninput`。而 `output` 列对应的内容为模型回答。

如果指定，`system` 列对应的内容将被作为系统提示词。

`history` 列是由多个字符串二元组构成的列表，分别代表历史消息中每轮对话的指令和回答。注意在指令监督微调时，历史消息中的回答内容也会被用于模型学习。

**JSON 样例：**

```json
{
  "instruction": "请解释一下量子纠缠的概念。",
  "input": "",
  "output": "量子纠缠是指两个或多个粒子在量子状态上相互关联的现象，即使它们相隔很远，一个粒子的状态变化会立即影响另一个粒子的状态。",
  "system": "你是一个知识渊博的助手。",
  "history": [
    ["什么是量子力学？", "量子力学是研究微观粒子行为的物理学分支。"]
  ]
}
```

##### **2.2 ShareGPT 格式**

适用于多轮对话的标准格式。

**样例数据集：**

在多轮对话微调时，`messages` 列对应的内容为一个包含多轮对话的列表，每个对话由 `role` 和 `content` 组成。`role` 表示消息的发送者（如 `user` 或 `assistant`），`content` 表示消息内容。

**JSON 样例：**

```json
{
  "messages": [
    {"role": "user", "content": "你好，能帮我解释一下黑洞吗？"},
    {"role": "assistant", "content": "黑洞是宇宙中一种具有极强引力的天体，连光都无法逃脱。"}
  ]
}
```

#### **3. 数据处理建议**

1. **数据审核**
   - 不要完全依赖 GPT 自动整理数据，需手动复核，剔除互斥或无效数据（脏数据）。
2. **数据简化**
   - 数据复杂度不宜过高，简化内容可提高训练效率。
3. **数据增强**
   - 数据增广（Data Augmentation）是提升模型性能的有效手段，建议根据实际需求灵活应用。
